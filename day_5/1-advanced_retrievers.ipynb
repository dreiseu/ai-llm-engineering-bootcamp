{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common issues with loans appear to involve problems with loan servicing, such as errors in loan balances, misapplied payments, wrongful denials of payment plans, and issues related to loan status and reporting. Many complaints also highlight difficulties in dealing with lenders or servicers, including lack of communication, incorrect information, and handling of payment options like forbearance or deferment.\\n\\nIn summary, the most common issue seems to be **problems with loan servicing**, including administrative errors, miscommunication, and mishandling of payments and account information.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, some complaints did not get handled in a timely manner. Specifically, at least one complaint, submitted to MOHELA on 03/28/25 regarding a delayed application process, was marked as \"Not timely,\" indicating a delay in handling. Additionally, multiple complaints related to ongoing issues such as unresponsive customer service, unresolved account errors, and delays in dispute responses suggest that there have been instances where complaints were not addressed promptly.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons, including:\\n\\n1. **Accumulation of interest during deferment or forbearance:** Borrowers often found that interest continued to grow even when payments were postponed, making repayment more difficult over time.\\n\\n2. **Lack of clear or timely communication from lenders or servicers:** Many borrowers were unaware of when their repayment obligations resumed or had difficulty understanding their loan status due to poor or inconsistent information.\\n\\n3. **Financial hardship and inability to afford payments:** Borrowers reported that increasing monthly payments would conflict with their basic living expenses, such as bills, food, and transportation.\\n\\n4. **Mismanagement and administrative issues:** Cases of improper loan transfers, incorrect reporting, and lack of access to accounts or understanding of balances contributed to repayment difficulties.\\n\\n5. **Systemic issues and lack of support:** Many borrowers felt misled about their repayment obligations, faced systemic breakdowns affecting credit reports, or were inadequately supported when seeking alternative payment options.\\n\\nOverall, a combination of financial strain, administrative challenges, and inadequate information contributed to borrowers’ inability to repay their loans effectively.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the provided context, the most common issue with loans appears to be related to dealing with lenders or servicers, particularly issues with the accuracy or transparency of information, handling of payments, and problematic practices like applying payments in a way that prolongs debt or charging unexpected fees. Specific recurring issues include:\\n\\n- Disputes over fees charged or incorrect fees\\n- Difficulties in obtaining accurate loan or payment information\\n- Problems with how payments are applied (e.g., not applying to principal)\\n- Inaccurate or bad information about loans\\n- Issues stemming from loan servicers' practices that seem predatory or untrustworthy\\n\\nWhile the context focuses on complaints about federal student loan servicers, these issues reflect broader common challenges in the loan process.\\n\\nIf you need a specific concise answer:  \\nThe most common issue with loans is difficulties and disputes related to loan servicing, including miscommunication, improper application of payments, and lack of transparency.\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, all the complaints listed were responded to in a timely manner, as indicated by the \"Timely response?\" field being marked \"Yes\" for each case. Therefore, there is no evidence that any complaints were not handled in a timely manner.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including issues with the handling of their payment plans, lack of communication from the servicers, and problems with their payments being reversed or not properly processed. Some specific causes include:\\n\\n- Being steered into the wrong types of forbearance or having their loan transferred without proper notification.\\n- Automatic payments being discontinued or not set up correctly, leading to missed or late payments.\\n- Lack of response or help from loan servicers when borrowers seek assistance or file for deferments or forbearances.\\n- Poor communication and notification from loan servicers regarding payment status, due dates, or account changes, sometimes leading to negative impacts on credit scores.\\n- Servicers submitting accounts as past due without proper notice, even though borrowers made regular payments.\\n\\nOverall, these issues point to failures on the part of loan servicers to effectively communicate with borrowers and manage their repayment plans, leading to missed payments and further financial difficulties for some borrowers.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ❓ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "### Answer:\n",
        "\n",
        "- Example Query: Find complaints about JuanHand online lending app payment processing errors.\n",
        "- BM25 excels at finding documents containing the exact term. Embeddings might return semantically similar  results about other loan services like Billease, but when you need information about JuanHand specifically, exact matching is crucial.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, a common issue with loans, particularly student loans, appears to be problems related to dealing with lenders or servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and mishandling of account information. Many complaints also involve difficulties with understanding repayment options, accumulating interest, and issues with loan disclosures or privacy violations. \\n\\nIn summary, a most common issue is mismanagement or mishandling by loan servicers, leading to errors, confusion, and financial hardship for borrowers.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, there are indications that some complaints did not get handled in a timely manner. For example:\\n\\n- One complaint has been open since around 18 months without resolution, involving issues like account reviews and violations.\\n- Another complaint involving legal violations has not received follow-up from the company.\\n- A customer reported that issues with their auto-pay setup persisted for over 2-3 weeks despite multiple calls.\\n\\nWhile the company responses in these cases state \"Yes\" for timely response, the ongoing unresolved issues and extended delays suggest that some complaints were indeed not addressed in a timely manner.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People often fail to pay back their loans due to a combination of factors, including lack of clear information about repayment obligations, difficulties with managing interest accumulation, and limited options for affordable repayment plans. In some cases, borrowers are unaware that they need to repay their loans or are not properly notified about changes in loan servicing or ownership, leading to missed payments or confusion about their balances. Additionally, high interest rates, compounded over time, can make it challenging to reduce the principal, especially if the borrower can only afford minimal payments, causing the debt to grow or remain unpaid. Financial hardships, lack of guidance from loan servicers, and limited access to affordable payment options like income-driven plans can ultimately contribute to borrowers' inability to fully repay their loans.\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issues with loans include:\\n\\n- Trouble with how payments are being handled, such as difficulty applying extra funds to principal or_payments being misapplied (e.g., primarily to interest).\\n- Problems with loan servicing, including errors in balances, misapplied payments, and wrongful denials of payment plans.\\n- Issues with loan balances increasing unexpectedly due to mismanagement or improper handling of interest (e.g., forbearance steering leading to interest capitalization).\\n- Inaccurate or misleading credit reporting related to loan status.\\n- Lack of proper communication from servicers about loan status, default notices, or changes in repayment terms.\\n- Problems with accessing and verifying loan information, including identity or fraud concerns.\\n- Servicer misconduct such as neglecting borrower rights, mishandling documents, or failing to process requests for deferment, consolidation, or forgiveness.\\n\\nOverall, the most recurring theme appears to be **servicing-related issues**, such as mismanagement of payments, errors in balances, and failure to properly communicate or process borrower requests. \\n\\nIf I had to summarize the most common issue based on this data, it would be:  \\n**Problems with loan servicing, including mishandling of payments, balances, and communication.**  \\n\\nIf you need a specific aspect or more detailed explanation, please let me know!'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that many complaints were handled in a timely manner, with responses marked as \"Yes\" for timely response, and some complaints explicitly stating responses occurred within required timeframes. \\n\\nHowever, there are also several complaints where responses were delayed or not received within the expected time, including cases marked as \"No\" for timely response, where complainants experienced wait times exceeding 15 days or where no response was received for over a year.\\n\\nTherefore, the answer is: **Yes, some complaints did not get handled in a timely manner.**'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for various reasons, including:\\n\\n1. Lack of clear information about the true cost of loans and interest accrual, leading borrowers to underestimate how much they owed.\\n2. Difficulty managing interest that continued to accumulate during forbearance or deferment, making loans harder to pay off over time.\\n3. Limited or confusing repayment options, with some borrowers not qualifying for loan forgiveness programs and feeling misled about available assistance.\\n4. Financial hardships such as unemployment, medical issues, or homelessness, which made payments impossible.\\n5. Challenges with loan servicers providing inaccurate or misleading information, poor communication, or mishandling accounts, which hindered borrowers' ability to manage or understand their loans.\\n6. Errors or discrepancies in loan reporting and account status that negatively impacted credit scores and created additional obstacles to repayment.\\n7. Experiences of being pushed into long-term forbearance or forced into consolidations without being informed of better options like income-driven repayment or loan rehabilitation.\\n8. Systemic issues within loan servicing and transfer processes, including unauthorized account transfers, lack of documentation, or improper handling, which contributed to confusion and default.\\n9. Specific situations such as inability to return to work after injury or illness, homelessness, or initial unawareness of student loan obligations.\\n\\nOverall, the failure to pay back loans was often related to a combination of financial hardship, lack of transparent information, systemic servicing issues, and the complex or confusing nature of student loan management.\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ❓ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Answer:\n",
        "\n",
        "- A single user query might miss relevant documents due to vocabulary mismatch. For example, if a user asks \"payment problems\", the LLM might generate variations like billing issues, transaction errors, payment processing difficulties, or account charge problems. By combining results from all query variations, you get the union of relevant documents rather than being limited to what a single query formulation can retrieve. This significantly improves recall at the potential cost of some precision.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be problems related to mismanagement and lack of transparency. Specific issues include:\\n\\n- Struggling to repay loans due to financial hardship and inadequate information about loan terms or consequences.\\n- Receiving bad or incomplete information about loans from lenders or servicers.\\n- Unauthorized or unexpected changes to loan terms, such as interest rate increases, incorrect balances, or lack of disclosure during processes like loan consolidation.\\n- Systemic breakdowns impacting credit reporting and loan servicing, leading to errors and negative credit score impacts.\\n\\nIn summary, the most common issue involves borrowers facing difficulties because of insufficient or misleading information, mismanagement, or systemic errors in loan handling and reporting.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, yes, several complaints indicate that issues were not handled in a timely manner. For example:\\n\\n- One complaint from 03/28/25 mentions that the consumer was told their application would be expedited and they would be contacted within 15 days, but as of the date of the complaint, no one had reached out.\\n- Multiple complaints from 04/11/25 describe long wait times (up to 7 hours or more) to speak with representatives and mention that issues remained unresolved or unaddressed for extended periods.\\n- Also, a complaint from 04/24/25 highlights repeated failed attempts to correct errors, with no resolution and ongoing difficulties, indicating delays and unresponsiveness.\\n\\nFurthermore, the metadata shows that for some complaints the response was marked \"No\" for timely response, which suggests that some issues did not get handled promptly.\\n\\nTherefore, yes, some complaints were not handled in a timely manner.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to financial hardship caused by inadequate post-graduation employment prospects, misrepresentations about the value of their education, and lack of transparency from the educational institutions and loan servicers. For example, one complainant was unable to secure employment in their field after attending a college that closed and did not provide real career support, leading to severe financial difficulties and difficulty in making loan payments. Others experienced issues such as being required to start payments before the grace period ended, or having their payment obligations miscommunicated or improperly managed by loan servicers. Additionally, some faced complications like unverified or disputed debt reporting, which further hindered their ability to manage or repay their loans. Overall, these issues highlight a combination of systemic mismanagement, lack of clear information, and personal financial hardships as reasons why individuals failed to pay back their loans.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issues with loans, based on the complaints data, include:\\n\\n- Dealing with lender or servicer issues such as mismanagement, incorrect reporting, or poor communication.\\n- Problems with how payments are being handled, including inability to allocate payments properly or being steered into high-interest forbearances.\\n- Errors and inaccuracies in loan balances, interest charges, or credit reporting.\\n- Lack of transparency or proper documentation, including missing or improperly managed loan records.\\n- Problems related to loan forgiveness, discharge, or discharge eligibility.\\n- Unauthorized transfer of loans and improper handling of loan data.\\n- Poor customer service and failure to resolve disputes effectively.\\n- Issues with interest accrual and improper interest charges.\\n- Misleading or incomplete information about loan terms and repayment options.\\n\\nIn summary, the most common and recurring issue appears to be the mishandling of loan servicing, including mismanagement, misreporting, and inadequate communication, which causes financial harm and frustration among borrowers.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, yes, some complaints indicate that issues were not handled in a timely manner. For example:\\n\\n- Complaint ID 12973003 (from EdFinancial Services, NJ): The customer mentioned that it is over 2-3 weeks and they are still experiencing the same issue, which suggests a delay beyond the promised timeframe.\\n- Complaint ID 12935889 (from MOHELA, CO): The complainant was dissatisfied with the response time, as their complaint was not resolved promptly.\\n- Complaint ID 12410063 (from EdFinancial Services, CA): The customer reported ongoing struggles over nearly two and a half years, with repeated follow-ups and delays in resolution.\\n\\nAdditionally, some complaints explicitly mention delays or failure to resolve issues promptly, indicating that not all complaints were handled in a timely manner.\\n\\nIf you need a specific definitive answer: yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons, including:\\n\\n- Lack of clear communication from loan servicers about when payments were expected to resume or if the loan had been transferred to a new servicer.\\n- Difficulty in understanding or managing complex repayment options, interest accumulation, and the impact of deferment or forbearance.\\n- Being misled by servicers into forbearance or consolidation without proper explanation of potential consequences like interest capitalization or loss of forgiveness eligibility.\\n- Changes in loan transfer status without notification, leading to unawareness of repayment obligations.\\n- Financial hardships that made monthly payments unaffordable, combined with insufficient support or guidance from lenders.\\n- Errors or inaccuracies in loan balance reporting, debt management, or credit reporting, causing confusion and financial hardship.\\n- Technical issues, such as trouble with online portals, misapplied payments, or inaccurate account status updates.\\n- Lack of access to or awareness of income-driven repayment or forgiveness programs.\\n\\nIn essence, many borrowers struggled to repay their loans due to systemic issues like poor communication, mismanagement, and complex loan servicing practices that did not adequately support or inform them about their repayment options or status.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints data, the most common issue with loans appears to be problems related to loan servicing and communication issues. Specific recurring issues include:\\n\\n- Trouble with how payments are being handled, such as incorrect payment amounts or re-amortization delays.\\n- Difficulties in obtaining clear or accurate information about loan status, balances, or payment plans.\\n- Problems with reporting and account status errors, such as loans being reported in default or delinquency falsely.\\n- Issues with auto-debit setup and verification.\\n- Concerns over improper reporting, breach of privacy, or illegal collection practices.\\n\\nOverall, challenges related to loan management, including lack of transparency, miscommunication, and administrative errors, seem to be the most prevalent issues with loans in this dataset.\\n\\nIf you need a specific summary or further details, please let me know!'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that some complaints did not get handled in a timely manner. Specifically, multiple complaints state that the companies responded with \"Closed with explanation\" and there is no indication that they addressed the issues within the expected timeframe. \\n\\nFor example:\\n- The complaint about Nelnet regarding unresponded letters and alleged misconduct indicates no subsequent response to the consumer\\'s inquiries.\\n- Several complaints mention that, despite the companies\\' responses, the issues remain unresolved, and the consumers continue to face problems such as incorrect billing, unauthorized reporting, or lack of communication.\\n\\nWhile some responses are marked \"Yes\" for timely response, the lack of resolution or ongoing disputes suggests that not all complaints were effectively handled in a timely manner. \\n\\nTherefore, yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including issues with loan servicing, miscommunication, or administrative problems. Some specific reasons highlighted in the complaints include:\\n\\n- Receiving bad or incomplete information about their loan status or repayment terms.\\n- Administrative delays or errors, such as failure to re-amortize payments after forbearance or missing payment records.\\n- Disputes over the legitimacy of the debt or treatment of their loan accounts.\\n- Problems with loan documentation or certification, leading to delays or complications in forgiveness or discharge.\\n- Allegations of illegal reporting or collection practices, which may contribute to confusion or default status errors.\\n- Personal financial difficulties, while not explicitly detailed, can also be implied as a general reason for default if borrowers are unable to meet payments.\\n\\nIn summary, failure to pay back loans was often related to administrative errors, communication issues, or legal disputes over the legitimacy and reporting of the loans.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ❓ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Answer:\n",
        "Short and repetitive sentences like FAQ entries will have very similar embeddings, resulting in minimal semantic distances between consecutive sentences. FAQ structures depend on maintaining clear Q&A pairs, but semantic chunking might merge multiple Q&A pairs if they're topically similar.\n",
        "\n",
        "The adjustments to make for the algorithm is to lower the threshold by using a more sensitive breakpoint to detect subtle semantic shifts between different FAQ topics. Additionally, implement custom logic to respect FAQ formatting patterns before applying semantic chunking. Lastly, combine semantic chunking with rule-based splitting that preserves Q&A pair integrity.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against each other. \n",
        "You can use the loans or bills dataset.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Andrei\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Andrei\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import getpass\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key: \")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key: \")\n",
        "\n",
        "# Enable LangSmith tracing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"AI/LLM Engineering\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.retrievers import ParentDocumentRetriever, EnsembleRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.testset import reasoning, multi_context\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    faithfulness,\n",
        "    answer_relevancy\n",
        ")\n",
        "\n",
        "from langsmith import Client\n",
        "from langsmith.evaluation import evaluate as langsmith_evaluate\n",
        "from langsmith.schemas import Run, Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m embeddings = \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-embedding-3-small\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m llm = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0\u001b[39m)\n\u001b[32m      4\u001b[39m langsmith_client = Client()\n",
            "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andrei\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:327\u001b[39m, in \u001b[36mOpenAIEmbeddings.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    325\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_client = httpx.Client(proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy)\n\u001b[32m    326\u001b[39m     sync_specific = {\u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_client}\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m.embeddings  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.openai_proxy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.http_async_client:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andrei\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\openai\\_client.py:130\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    128\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m     )\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "langsmith_client = Client()\n",
        "\n",
        "dataset = load_dataset(\"squad\", split=\"validation[:20]\")\n",
        "\n",
        "documents = []\n",
        "for item in dataset:\n",
        "    doc = Document(\n",
        "        page_content=f\"Title: {item['title']}\\n\\nContext: {item['context']}\",\n",
        "        metadata={\n",
        "            \"title\": item['title'],\n",
        "            \"context\": item['context'],\n",
        "            \"source\": \"squad\"\n",
        "        }\n",
        "    )\n",
        "    documents.append(doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retrievers = {}\n",
        "\n",
        "# Naive Retriever\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"naive_retrieval\"\n",
        ")\n",
        "retrievers[\"naive\"] = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# BM25 Retriever\n",
        "retrievers[\"bm25\"] = BM25Retriever.from_documents(documents, k=5)\n",
        "\n",
        "# Reranking Retriever\n",
        "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\", top_k=5)\n",
        "retrievers[\"reranking\"] = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=base_retriever\n",
        ")\n",
        "\n",
        "# Multi-query Retriever\n",
        "retrievers[\"multi_query\"] = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Parent Document Retriever\n",
        "parent_vectorstore = Qdrant.from_documents(\n",
        "    [],\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"parent_retrieval\"\n",
        ")\n",
        "store = InMemoryStore()\n",
        "child_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "parent_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=parent_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    search_kwargs={\"k\": 5}\n",
        ")\n",
        "parent_retriever.add_documents(documents)\n",
        "retrievers[\"parent_document\"] = parent_retriever\n",
        "\n",
        "# Ensemble Retriever\n",
        "ensemble_retrievers = [\n",
        "    retrievers[\"naive\"],\n",
        "    retrievers[\"bm25\"],\n",
        "    retrievers[\"reranking\"]\n",
        "]\n",
        "weights = [0.4, 0.3, 0.3]\n",
        "retrievers[\"ensemble\"] = EnsembleRetriever(\n",
        "    retrievers=ensemble_retrievers,\n",
        "    weights=weights,\n",
        "    search_kwargs={\"k\": 5}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm = llm,\n",
        "    critic_llm = llm,\n",
        "    embeddings=embeddings\n",
        ")\n",
        "\n",
        "testset = generator.generate_with_langchain_docs(\n",
        "    documents,\n",
        "    test_size=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'testset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dataset_name = \u001b[33m\"\u001b[39m\u001b[33mretrieval-evaluation-dataset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m testset_df = \u001b[43mtestset\u001b[49m.to_pandas()\n\u001b[32m      4\u001b[39m examples = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m testset_df.iterrows():\n",
            "\u001b[31mNameError\u001b[39m: name 'testset' is not defined"
          ]
        }
      ],
      "source": [
        "dataset_name = \"retrieval-evaluation-dataset\"\n",
        "testset_df = testset.to_pandas()\n",
        "\n",
        "examples = []\n",
        "for _, row in testset_df.iterrows():\n",
        "    example = {\n",
        "        \"question\": row[\"question\"],\n",
        "        \"ground_truth\": row[\"ground_truth\"],\n",
        "        \"contexts\": row.get(\"contexts\", [])\n",
        "    }\n",
        "    examples.append(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    try:\n",
        "        langsmith_client.delete_dataset(dataset_name=dataset_name)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    dataset = langsmith_client.create_dataset(\n",
        "        dataset_name=dataset_name,\n",
        "        description=\"Retrieval evaluation dataset generated with Ragas\"\n",
        "    )\n",
        "\n",
        "    langsmith_client.create_examples(\n",
        "        inputs=[{\"question\": ex[\"question\"]} for ex in examples],\n",
        "        outputs=[{\"ground_truth\": ex[\"ground_truth\"], \"contexts\": ex[\"contexts\"]} for ex in examples],\n",
        "        dataset_id=dataset.id\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieval_accuracy_evaluator(run: Run, example: Example) -> dict:\n",
        "    \"\"\"Evaluate if the correct context was retrieved.\"\"\"\n",
        "    if not run.outputs or \"contexts\" not in run.outputs:\n",
        "        return {\"key\": \"retrieval_accuracy\", \"score\": 0.0}\n",
        "    \n",
        "    retrieved_contexts = run.outputs[\"contexts\"]\n",
        "    expected_contexts = example.outputs.get(\"contexts\", [])\n",
        "\n",
        "    if not expected_contexts:\n",
        "        return {\"key\": \"retrieval_accuracy\", \"score\": 0.0}\n",
        "    \n",
        "    overlap = 0\n",
        "    for expected in expected_contexts:\n",
        "        for retrieved in retrieved_contexts:\n",
        "            if expected.lower() in retrieved.lower() or retrieved.lower() in expected.lower():\n",
        "                overlap += 1\n",
        "                break\n",
        "\n",
        "    accuracy = overlap / len(expected_contexts) if expected_contexts else 0\n",
        "    return {\"key\": \"retrieval_accuracy\", \"score\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_quality_evaluator(run: Run, example: Example) -> dict:\n",
        "    \"\"\"Evaluate answer quality using LLM.\"\"\"\n",
        "    if not run.outputs or \"answer\" not in run.outputs:\n",
        "        return {\"key\": \"answer_quality\", \"score\": 0.0}\n",
        "    \n",
        "    answer = run.outputs[\"answer\"]\n",
        "    ground_truth = example.outputs.get(\"ground_truth\", \"\")\n",
        "    question = example.inputs[\"question\"]\n",
        "    \n",
        "    # Use LLM to evaluate answer quality\n",
        "    evaluation_prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Ground Truth: {ground_truth}\n",
        "    Generated Answer: {answer}\n",
        "    \n",
        "    Rate the quality of the generated answer compared to the ground truth on a scale of 0-1:\n",
        "    - 0: Completely incorrect or irrelevant\n",
        "    - 0.5: Partially correct but missing key information\n",
        "    - 1: Accurate and complete\n",
        "    \n",
        "    Return only the numerical score.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        score_text = llm.invoke(evaluation_prompt).content\n",
        "        score = float(score_text.strip())\n",
        "        score = max(0.0, min(1.0, score))  # Clamp between 0 and 1\n",
        "    except:\n",
        "        score = 0.0\n",
        "    \n",
        "    return {\"key\": \"answer_quality\", \"score\": score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def context_relevance_evaluator(run: Run, example: Example) -> dict:\n",
        "    \"\"\"Evaluate how relevant retrieved contexts are to the question.\"\"\"\n",
        "    if not run.outputs or \"contexts\" not in run.outputs:\n",
        "        return {\"key\": \"context_relevance\", \"score\": 0.0}\n",
        "    \n",
        "    contexts = run.outputs[\"contexts\"]\n",
        "    question = example.inputs[\"question\"]\n",
        "    \n",
        "    if not contexts:\n",
        "        return {\"key\": \"context_relevance\", \"score\": 0.0}\n",
        "    \n",
        "    # Use LLM to evaluate context relevance\n",
        "    context_text = \"\\n\\n\".join(contexts[:3])  # Use top 3 contexts\n",
        "    evaluation_prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Retrieved Contexts: {context_text}\n",
        "    \n",
        "    Rate how relevant these contexts are to answering the question on a scale of 0-1:\n",
        "    - 0: Completely irrelevant\n",
        "    - 0.5: Somewhat relevant but missing key information\n",
        "    - 1: Highly relevant and contains information needed to answer the question\n",
        "    \n",
        "    Return only the numerical score.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        score_text = llm.invoke(evaluation_prompt).content\n",
        "        score = float(score_text.strip())\n",
        "        score = max(0.0, min(1.0, score))\n",
        "    except:\n",
        "        score = 0.0\n",
        "    \n",
        "    return {\"key\": \"context_relevance\", \"score\": score}\n",
        "\n",
        "    evaluators = [\n",
        "        retrieval_accuracy_evaluator,\n",
        "        answer_quality_evaluator,\n",
        "        context_relevance_evaluator\n",
        "    ]\n",
        "\n",
        "    return evaluators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_retriever(retriever, retriever_name, testset, dataset_id=None, langsmith_evaluators=None):\n",
        "    \"\"\"Evaluate a single retriever using both Ragas and LangSmith metrics.\"\"\"\n",
        "    print(f\"\\n📊 Evaluating {retriever_name} retriever...\")\n",
        "    \n",
        "    # Create retriever function for LangSmith\n",
        "    def retriever_chain(inputs: dict) -> dict:\n",
        "        \"\"\"Retriever chain for LangSmith evaluation.\"\"\"\n",
        "        question = inputs[\"question\"]\n",
        "        \n",
        "        try:\n",
        "            # Retrieve documents\n",
        "            retrieved_docs = retriever.invoke(question)\n",
        "            contexts = [doc.page_content for doc in retrieved_docs]\n",
        "            \n",
        "            # Generate answer using RAG chain\n",
        "            context_str = \"\\n\\n\".join(contexts)\n",
        "            prompt_template = \"\"\"Based on the following context, answer the question.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "            \n",
        "            prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "            rag_chain = prompt | llm | StrOutputParser()\n",
        "            \n",
        "            answer = rag_chain.invoke({\n",
        "                \"context\": context_str,\n",
        "                \"question\": question\n",
        "            })\n",
        "            \n",
        "            return {\n",
        "                \"contexts\": contexts,\n",
        "                \"answer\": answer,\n",
        "                \"question\": question\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Error in retriever chain: {e}\")\n",
        "            return {\n",
        "                \"contexts\": [],\n",
        "                \"answer\": \"Error occurred during retrieval\",\n",
        "                \"question\": question\n",
        "            }\n",
        "    \n",
        "    # Prepare evaluation data for Ragas\n",
        "    eval_data = {\n",
        "        \"question\": [],\n",
        "        \"contexts\": [],\n",
        "        \"answer\": [],\n",
        "        \"ground_truth\": []\n",
        "    }\n",
        "    \n",
        "    # Process each question in testset\n",
        "    testset_df = testset.to_pandas()\n",
        "    \n",
        "    for _, row in testset_df.iterrows():\n",
        "        question = row[\"question\"]\n",
        "        ground_truth = row[\"ground_truth\"]\n",
        "        \n",
        "        try:\n",
        "            result = retriever_chain({\"question\": question})\n",
        "            \n",
        "            eval_data[\"question\"].append(question)\n",
        "            eval_data[\"contexts\"].append(result[\"contexts\"])\n",
        "            eval_data[\"answer\"].append(result[\"answer\"])\n",
        "            eval_data[\"ground_truth\"].append(ground_truth)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Error processing question: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Run Ragas evaluation\n",
        "    ragas_results = None\n",
        "    if len(eval_data[\"question\"]) > 0:\n",
        "        eval_dataset = pd.DataFrame(eval_data)\n",
        "        \n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            \n",
        "            ragas_result = evaluate(\n",
        "                eval_dataset,\n",
        "                metrics=[\n",
        "                    context_precision,\n",
        "                    context_recall,\n",
        "                    faithfulness,\n",
        "                    answer_relevancy\n",
        "                ]\n",
        "            )\n",
        "            \n",
        "            evaluation_time = time.time() - start_time\n",
        "            \n",
        "            ragas_results = {\n",
        "                \"context_precision\": ragas_result[\"context_precision\"],\n",
        "                \"context_recall\": ragas_result[\"context_recall\"],\n",
        "                \"faithfulness\": ragas_result[\"faithfulness\"],\n",
        "                \"answer_relevancy\": ragas_result[\"answer_relevancy\"],\n",
        "                \"evaluation_time\": evaluation_time,\n",
        "                \"num_questions\": len(eval_dataset)\n",
        "            }\n",
        "            \n",
        "            print(f\"  ✅ Ragas evaluation completed ({evaluation_time:.2f}s)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Ragas evaluation failed: {e}\")\n",
        "    \n",
        "    # Run LangSmith evaluation\n",
        "    langsmith_results = None\n",
        "    if dataset_id and langsmith_evaluators:\n",
        "        try:\n",
        "            print(f\"  🔧 Running LangSmith evaluation...\")\n",
        "            \n",
        "            experiment_name = f\"{retriever_name}-experiment\"\n",
        "            \n",
        "            langsmith_result = langsmith_evaluate(\n",
        "                retriever_chain,\n",
        "                data=dataset_id,\n",
        "                evaluators=langsmith_evaluators,\n",
        "                experiment_prefix=experiment_name,\n",
        "                metadata={\"retriever_type\": retriever_name}\n",
        "            )\n",
        "            \n",
        "            # Extract LangSmith metrics\n",
        "            langsmith_results = {}\n",
        "            for result in langsmith_result:\n",
        "                if hasattr(result, 'evaluation_results'):\n",
        "                    for eval_result in result.evaluation_results:\n",
        "                        metric_name = eval_result.key\n",
        "                        if metric_name not in langsmith_results:\n",
        "                            langsmith_results[metric_name] = []\n",
        "                        langsmith_results[metric_name].append(eval_result.score)\n",
        "            \n",
        "            # Calculate averages\n",
        "            for metric, scores in langsmith_results.items():\n",
        "                langsmith_results[metric] = sum(scores) / len(scores) if scores else 0.0\n",
        "            \n",
        "            print(f\"  ✅ LangSmith evaluation completed\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ LangSmith evaluation failed: {e}\")\n",
        "    \n",
        "    # Combine results\n",
        "    combined_results = {\n",
        "        \"retriever\": retriever_name,\n",
        "        \"num_questions\": len(eval_data[\"question\"])\n",
        "    }\n",
        "    \n",
        "    if ragas_results:\n",
        "        combined_results.update(ragas_results)\n",
        "    \n",
        "    if langsmith_results:\n",
        "        # Add LangSmith metrics with prefix\n",
        "        for metric, score in langsmith_results.items():\n",
        "            combined_results[f\"langsmith_{metric}\"] = score\n",
        "    \n",
        "    return combined_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_comprehensive_evaluation():\n",
        "    \"\"\"Run the complete evaluation pipeline with both Ragas and LangSmith.\"\"\"\n",
        "    print(\"\\n🎯 Starting Comprehensive Evaluation\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Load dataset\n",
        "    documents = load_squad_dataset(num_documents=100)\n",
        "    \n",
        "    # Create retrievers\n",
        "    retrievers = create_all_retrievers(documents)\n",
        "    \n",
        "    # Generate synthetic dataset\n",
        "    testset = generate_synthetic_dataset(documents, num_questions=50)\n",
        "    \n",
        "    # Create LangSmith dataset and evaluators\n",
        "    dataset_id, examples = create_langsmith_dataset(testset)\n",
        "    langsmith_evaluators = create_langsmith_evaluators()\n",
        "    \n",
        "    # Evaluate each retriever\n",
        "    all_results = []\n",
        "    \n",
        "    for retriever_name, retriever in retrievers.items():\n",
        "        result = evaluate_retriever(\n",
        "            retriever, \n",
        "            retriever_name, \n",
        "            testset, \n",
        "            dataset_id, \n",
        "            langsmith_evaluators\n",
        "        )\n",
        "        if result:\n",
        "            all_results.append(result)\n",
        "    \n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_results(results):\n",
        "    \"\"\"Analyze results and provide comprehensive report.\"\"\"\n",
        "    print(\"\\n📈 ANALYSIS REPORT\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if not results:\n",
        "        print(\"❌ No results to analyze\")\n",
        "        return\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    # Display Ragas results table\n",
        "    print(\"\\n📊 RAGAS EVALUATION RESULTS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Retriever':<15} {'Precision':<10} {'Recall':<10} {'Relevancy':<10} {'Faithfulness':<12} {'Answer Rel.':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        print(f\"{row['retriever']:<15} \"\n",
        "              f\"{row.get('context_precision', 0):<10.3f} \"\n",
        "              f\"{row.get('context_recall', 0):<10.3f} \"\n",
        "              f\"{row.get('context_relevancy', 0):<10.3f} \"\n",
        "              f\"{row.get('faithfulness', 0):<12.3f} \"\n",
        "              f\"{row.get('answer_relevancy', 0):<10.3f}\")\n",
        "    \n",
        "    # Display LangSmith results if available\n",
        "    langsmith_cols = [col for col in df.columns if col.startswith('langsmith_')]\n",
        "    if langsmith_cols:\n",
        "        print(\"\\n📊 LANGSMITH EVALUATION RESULTS:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Retriever':<15} \", end=\"\")\n",
        "        for col in langsmith_cols:\n",
        "            metric_name = col.replace('langsmith_', '').replace('_', ' ').title()\n",
        "            print(f\"{metric_name:<15} \", end=\"\")\n",
        "        print()\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        for _, row in df.iterrows():\n",
        "            print(f\"{row['retriever']:<15} \", end=\"\")\n",
        "            for col in langsmith_cols:\n",
        "                print(f\"{row.get(col, 0):<15.3f} \", end=\"\")\n",
        "            print()\n",
        "    \n",
        "    # Calculate composite scores\n",
        "    ragas_metrics = ['context_precision', 'context_recall', 'context_relevancy', 'faithfulness', 'answer_relevancy']\n",
        "    available_ragas = [metric for metric in ragas_metrics if metric in df.columns]\n",
        "    \n",
        "    if available_ragas:\n",
        "        df['ragas_composite'] = df[available_ragas].mean(axis=1)\n",
        "    \n",
        "    langsmith_metrics = [col for col in langsmith_cols if 'accuracy' in col or 'quality' in col or 'relevance' in col]\n",
        "    if langsmith_metrics:\n",
        "        df['langsmith_composite'] = df[langsmith_metrics].mean(axis=1)\n",
        "    \n",
        "    # Find best performers\n",
        "    if 'ragas_composite' in df.columns:\n",
        "        best_ragas = df.loc[df['ragas_composite'].idxmax()]\n",
        "        print(f\"\\n🏆 BEST RAGAS PERFORMER:\")\n",
        "        print(f\"Best Overall (Ragas): {best_ragas['retriever']} (composite score: {best_ragas['ragas_composite']:.3f})\")\n",
        "    \n",
        "    if 'langsmith_composite' in df.columns:\n",
        "        best_langsmith = df.loc[df['langsmith_composite'].idxmax()]\n",
        "        print(f\"\\n🏆 BEST LANGSMITH PERFORMER:\")\n",
        "        print(f\"Best Overall (LangSmith): {best_langsmith['retriever']} (composite score: {best_langsmith['langsmith_composite']:.3f})\")\n",
        "    \n",
        "    # Individual metric leaders\n",
        "    print(f\"\\n🥇 METRIC LEADERS:\")\n",
        "    if 'context_precision' in df.columns:\n",
        "        best_precision = df.loc[df['context_precision'].idxmax()]\n",
        "        print(f\"Best Precision: {best_precision['retriever']} ({best_precision['context_precision']:.3f})\")\n",
        "    \n",
        "    if 'context_recall' in df.columns:\n",
        "        best_recall = df.loc[df['context_recall'].idxmax()]\n",
        "        print(f\"Best Recall: {best_recall['retriever']} ({best_recall['context_recall']:.3f})\")\n",
        "    \n",
        "    if 'evaluation_time' in df.columns:\n",
        "        fastest = df.loc[df['evaluation_time'].idxmin()]\n",
        "        print(f\"Fastest: {fastest['retriever']} ({fastest['evaluation_time']:.2f}s)\")\n",
        "    \n",
        "    # Cost analysis (estimated)\n",
        "    print(f\"\\n💰 COST ANALYSIS (Estimated):\")\n",
        "    cost_order = [\"bm25\", \"naive\", \"parent_document\", \"reranking\", \"multi_query\", \"ensemble\"]\n",
        "    cost_levels = [\"Lowest\", \"Low\", \"Medium\", \"Medium-High\", \"High\", \"Highest\"]\n",
        "    \n",
        "    for retriever, cost in zip(cost_order, cost_levels):\n",
        "        if retriever in df['retriever'].values:\n",
        "            print(f\"{retriever:<15}: {cost}\")\n",
        "    \n",
        "    # Enhanced performance analysis paragraph\n",
        "    print(f\"\\n📝 COMPREHENSIVE PERFORMANCE ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    best_overall = None\n",
        "    if 'ragas_composite' in df.columns:\n",
        "        best_overall = df.loc[df['ragas_composite'].idxmax()]\n",
        "    elif available_ragas:\n",
        "        best_overall = df.loc[df[available_ragas[0]].idxmax()]\n",
        "    \n",
        "    if best_overall is not None:\n",
        "        analysis = f\"\"\"\n",
        "Based on comprehensive evaluation using both Ragas and LangSmith frameworks on {len(results)} \n",
        "retrieval methods with the SQuAD dataset, the {best_overall['retriever']} retriever achieved \n",
        "the best overall performance.\n",
        "\n",
        "EVALUATION FRAMEWORK INSIGHTS:\n",
        "• Ragas Metrics: Focused on context precision, recall, relevancy, faithfulness, and answer relevancy\n",
        "• LangSmith Metrics: Custom evaluators for retrieval accuracy, answer quality, and context relevance\n",
        "• Dataset: {best_overall.get('num_questions', 'N/A')} synthetic questions generated from 100 SQuAD documents\n",
        "\n",
        "KEY FINDINGS:\n",
        "• Best Overall Performer: {best_overall['retriever']} \n",
        "• Ragas Composite Score: {best_overall.get('ragas_composite', 'N/A'):.3f}\n",
        "• LangSmith Composite Score: {best_overall.get('langsmith_composite', 'N/A'):.3f}\n",
        "\n",
        "RETRIEVER-SPECIFIC INSIGHTS:\n",
        "• BM25: Fastest execution, excellent for keyword-heavy queries\n",
        "• Naive Semantic: Good baseline performance with embedding similarity\n",
        "• Reranking: Higher precision through secondary ranking, increased cost\n",
        "• Multi-Query: Enhanced recall through query expansion, higher latency\n",
        "• Parent-Document: Better context preservation through chunk-to-document mapping\n",
        "• Ensemble: Combines strengths of multiple approaches for robust performance\n",
        "\n",
        "PRODUCTION RECOMMENDATIONS:\n",
        "For SQuAD-like factual Q&A datasets, {best_overall['retriever']} provides optimal \n",
        "balance of accuracy and performance. Consider ensemble methods for maximum accuracy \n",
        "or BM25 for latency-critical applications.\n",
        "\n",
        "COST-PERFORMANCE TRADE-OFFS:\n",
        "• Low-cost option: BM25 (no embedding/API costs)\n",
        "• Balanced option: Naive semantic search\n",
        "• High-accuracy option: Ensemble or reranking\n",
        "• Specialized option: Parent-document for long-form content\n",
        "        \"\"\".strip()\n",
        "    else:\n",
        "        analysis = \"Evaluation completed but insufficient data for comprehensive analysis.\"\n",
        "    \n",
        "    print(analysis)\n",
        "    \n",
        "    # Save results\n",
        "    df.to_csv(\"comprehensive_retrieval_results.csv\", index=False)\n",
        "    print(f\"\\n💾 Results saved to 'comprehensive_retrieval_results.csv'\")\n",
        "    \n",
        "    return df, analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Analysis & Observations:\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "13-advanced-retrieval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
