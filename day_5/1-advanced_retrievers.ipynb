{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-IqJAMkwnCF"
   },
   "source": [
    "# Advanced Retrieval with LangChain\n",
    "\n",
    "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
    "\n",
    "We'll touch on:\n",
    "\n",
    "- Naive Retrieval\n",
    "- Best-Matching 25 (BM25)\n",
    "- Multi-Query Retrieval\n",
    "- Parent-Document Retrieval\n",
    "- Contextual Compression (a.k.a. Rerank)\n",
    "- Ensemble Retrieval\n",
    "- Semantic chunking\n",
    "\n",
    "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
    "\n",
    "There will be two breakout rooms:\n",
    "\n",
    "- ü§ù Breakout Room Part #1\n",
    "  - Task 1: Getting Dependencies!\n",
    "  - Task 2: Data Collection and Preparation\n",
    "  - Task 3: Setting Up QDrant!\n",
    "  - Task 4-10: Retrieval Strategies\n",
    "- ü§ù Breakout Room Part #2\n",
    "  - Activity: Evaluate with Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rKP3hgHivpe"
   },
   "source": [
    "# ü§ù Breakout Room Part #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xes8oT-xHN7"
   },
   "source": [
    "## Task 1: Getting Dependencies!\n",
    "\n",
    "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7OHJXzfyJyA"
   },
   "source": [
    "We'll also provide our OpenAI key, as well as our Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LttlDQUYgSI",
    "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iUahNiJyQbv",
    "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
   },
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw304iAFyRtl"
   },
   "source": [
    "## Task 2: Data Collection and Preparation\n",
    "\n",
    "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A92NC2QZzCsi"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GshBjVRJZ6p8"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=f\"./data/complaints.csv\",\n",
    "    metadata_columns=[\n",
    "      \"Date received\", \n",
    "      \"Product\", \n",
    "      \"Sub-product\", \n",
    "      \"Issue\", \n",
    "      \"Sub-issue\", \n",
    "      \"Consumer complaint narrative\", \n",
    "      \"Company public response\", \n",
    "      \"Company\", \n",
    "      \"State\", \n",
    "      \"ZIP code\", \n",
    "      \"Tags\", \n",
    "      \"Consumer consent provided?\", \n",
    "      \"Submitted via\", \n",
    "      \"Date sent to company\", \n",
    "      \"Company response to consumer\", \n",
    "      \"Timely response?\", \n",
    "      \"Consumer disputed?\", \n",
    "      \"Complaint ID\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "loan_complaint_data = loader.load()\n",
    "\n",
    "for doc in loan_complaint_data:\n",
    "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gQphb6y0C0S"
   },
   "source": [
    "Let's look at an example document to see if everything worked as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkUkCf7DaMiq",
    "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_complaint_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWaQpdHl0Gzc"
   },
   "source": [
    "## Task 3: Setting up QDrant!\n",
    "\n",
    "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
    "\n",
    "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
    "\n",
    "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NT8ihRJbYmMT"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    loan_complaint_data,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LoanComplaints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x2SS4Rh0hiN"
   },
   "source": [
    "## Task 4: Naive RAG Chain\n",
    "\n",
    "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEH7X5Ai08FH"
   },
   "source": [
    "### R - Retrieval\n",
    "\n",
    "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
    "\n",
    "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GFDPrNBtb72o"
   },
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbBhyQjz06dx"
   },
   "source": [
    "### A - Augmented\n",
    "\n",
    "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7uSz-Dbqcoki"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlRzpb231GGJ"
   },
   "source": [
    "### G - Generation\n",
    "\n",
    "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c-1t9H60dJLg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg3QRGzA1M2x"
   },
   "source": [
    "### LCEL RAG Chain\n",
    "\n",
    "We're going to use LCEL to construct our chain.\n",
    "\n",
    "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0bvstS7mdOW3"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izKujhNb1ZG8"
   },
   "source": [
    "Let's see how this simple chain does on a few different prompts.\n",
    "\n",
    "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LI-5ueEddku9",
    "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, the most common issues with loans appear to involve problems with loan servicing, such as errors in loan balances, misapplied payments, wrongful denials of payment plans, and issues related to loan status and reporting. Many complaints also highlight difficulties in dealing with lenders or servicers, including lack of communication, incorrect information, and handling of payment options like forbearance or deferment.\\n\\nIn summary, the most common issue seems to be **problems with loan servicing**, including administrative errors, miscommunication, and mishandling of payments and account information.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "43zdcdUydtXh",
    "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, some complaints did not get handled in a timely manner. Specifically, at least one complaint, submitted to MOHELA on 03/28/25 regarding a delayed application process, was marked as \"Not timely,\" indicating a delay in handling. Additionally, multiple complaints related to ongoing issues such as unresponsive customer service, unresolved account errors, and delays in dispute responses suggest that there have been instances where complaints were not addressed promptly.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "lpG6rlvvvKFq",
    "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans for several reasons, including:\\n\\n1. **Accumulation of interest during deferment or forbearance:** Borrowers often found that interest continued to grow even when payments were postponed, making repayment more difficult over time.\\n\\n2. **Lack of clear or timely communication from lenders or servicers:** Many borrowers were unaware of when their repayment obligations resumed or had difficulty understanding their loan status due to poor or inconsistent information.\\n\\n3. **Financial hardship and inability to afford payments:** Borrowers reported that increasing monthly payments would conflict with their basic living expenses, such as bills, food, and transportation.\\n\\n4. **Mismanagement and administrative issues:** Cases of improper loan transfers, incorrect reporting, and lack of access to accounts or understanding of balances contributed to repayment difficulties.\\n\\n5. **Systemic issues and lack of support:** Many borrowers felt misled about their repayment obligations, faced systemic breakdowns affecting credit reports, or were inadequately supported when seeking alternative payment options.\\n\\nOverall, a combination of financial strain, administrative challenges, and inadequate information contributed to borrowers‚Äô inability to repay their loans effectively.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsbfQmbr1leg"
   },
   "source": [
    "Overall, this is not bad! Let's see if we can make it better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft1vt8HPR16w"
   },
   "source": [
    "## Task 5: Best-Matching 25 (BM25) Retriever\n",
    "\n",
    "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
    "\n",
    "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
    "\n",
    "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qdF4wuj5R-cG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIjJlBQ8drKH"
   },
   "source": [
    "We'll construct the same chain - only changing the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WR15EQG7SLuw"
   },
   "outputs": [],
   "source": [
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gi-yXCDdvJk"
   },
   "source": [
    "Let's look at the responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "oY9qzmm3SOrF",
    "outputId": "4d4f450f-5978-460f-f242-b32407868353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, the most common issue with loans appears to be related to dealing with lenders or servicers, particularly issues with the accuracy or transparency of information, handling of payments, and problematic practices like applying payments in a way that prolongs debt or charging unexpected fees. Specific recurring issues include:\\n\\n- Disputes over fees charged or incorrect fees\\n- Difficulties in obtaining accurate loan or payment information\\n- Problems with how payments are applied (e.g., not applying to principal)\\n- Inaccurate or bad information about loans\\n- Issues stemming from loan servicers' practices that seem predatory or untrustworthy\\n\\nWhile the context focuses on complaints about federal student loan servicers, these issues reflect broader common challenges in the loan process.\\n\\nIf you need a specific concise answer:  \\nThe most common issue with loans is difficulties and disputes related to loan servicing, including miscommunication, improper application of payments, and lack of transparency.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "igfinyneSQkh",
    "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, all the complaints listed were responded to in a timely manner, as indicated by the \"Timely response?\" field being marked \"Yes\" for each case. Therefore, there is no evidence that any complaints were not handled in a timely manner.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "w0H7pV_USSMQ",
    "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans for various reasons, including issues with the handling of their payment plans, lack of communication from the servicers, and problems with their payments being reversed or not properly processed. Some specific causes include:\\n\\n- Being steered into the wrong types of forbearance or having their loan transferred without proper notification.\\n- Automatic payments being discontinued or not set up correctly, leading to missed or late payments.\\n- Lack of response or help from loan servicers when borrowers seek assistance or file for deferments or forbearances.\\n- Poor communication and notification from loan servicers regarding payment status, due dates, or account changes, sometimes leading to negative impacts on credit scores.\\n- Servicers submitting accounts as past due without proper notice, even though borrowers made regular payments.\\n\\nOverall, these issues point to failures on the part of loan servicers to effectively communicate with borrowers and manage their repayment plans, leading to missed payments and further financial difficulties for some borrowers.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvg5xHaUdxCl"
   },
   "source": [
    "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### ‚ùì Question #1:\n",
    "\n",
    "Give an example query where BM25 is better than embeddings and justify your answer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "### Answer:\n",
    "\n",
    "- Example Query: Find complaints about JuanHand online lending app payment processing errors.\n",
    "- BM25 excels at finding documents containing the exact term. Embeddings might return semantically similar  results about other loan services like Billease, but when you need information about JuanHand specifically, exact matching is crucial.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-dcbFn2vpZF"
   },
   "source": [
    "## Task 6: Contextual Compression (Using Reranking)\n",
    "\n",
    "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
    "\n",
    "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
    "\n",
    "The basic idea here is this:\n",
    "\n",
    "- We retrieve lots of documents that are very likely related to our query vector\n",
    "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
    "\n",
    "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
    "\n",
    "All we need to do is the following:\n",
    "\n",
    "- Create a basic retriever\n",
    "- Create a compressor (reranker, in this case)\n",
    "\n",
    "That's it!\n",
    "\n",
    "Let's see it in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "psHvO2K1v_ZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TA9RB2x-j7P"
   },
   "source": [
    "Let's create our chain again, and see how this does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1BXqmxvHwX6T"
   },
   "outputs": [],
   "source": [
    "contextual_compression_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V3iGpokswcBb",
    "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, a common issue with loans, particularly student loans, appears to be problems related to dealing with lenders or servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and mishandling of account information. Many complaints also involve difficulties with understanding repayment options, accumulating interest, and issues with loan disclosures or privacy violations. \\n\\nIn summary, a most common issue is mismanagement or mishandling by loan servicers, leading to errors, confusion, and financial hardship for borrowers.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "7u_k0i4OweUd",
    "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, there are indications that some complaints did not get handled in a timely manner. For example:\\n\\n- One complaint has been open since around 18 months without resolution, involving issues like account reviews and violations.\\n- Another complaint involving legal violations has not received follow-up from the company.\\n- A customer reported that issues with their auto-pay setup persisted for over 2-3 weeks despite multiple calls.\\n\\nWhile the company responses in these cases state \"Yes\" for timely response, the ongoing unresolved issues and extended delays suggest that some complaints were indeed not addressed in a timely manner.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "zn1EqaGqweXN",
    "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"People often fail to pay back their loans due to a combination of factors, including lack of clear information about repayment obligations, difficulties with managing interest accumulation, and limited options for affordable repayment plans. In some cases, borrowers are unaware that they need to repay their loans or are not properly notified about changes in loan servicing or ownership, leading to missed payments or confusion about their balances. Additionally, high interest rates, compounded over time, can make it challenging to reduce the principal, especially if the borrower can only afford minimal payments, causing the debt to grow or remain unpaid. Financial hardships, lack of guidance from loan servicers, and limited access to affordable payment options like income-driven plans can ultimately contribute to borrowers' inability to fully repay their loans.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEbT0g2S-mZ4"
   },
   "source": [
    "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqbghrBEQNn5"
   },
   "source": [
    "## Task 7: Multi-Query Retriever\n",
    "\n",
    "Typically in RAG we have a single query - the one provided by the user.\n",
    "\n",
    "What if we had....more than one query!\n",
    "\n",
    "In essence, a Multi-Query Retriever works by:\n",
    "\n",
    "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
    "2. Retrieving documents for each query.\n",
    "3. Using all unique retrieved documents as context\n",
    "\n",
    "So, how is it to set-up? Not bad! Let's see it down below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pfM26ReXQjzU"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1vRc129jQ5WW"
   },
   "outputs": [],
   "source": [
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "CGgNuOb3Q3M9",
    "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the most common issues with loans include:\\n\\n- Trouble with how payments are being handled, such as difficulty applying extra funds to principal or_payments being misapplied (e.g., primarily to interest).\\n- Problems with loan servicing, including errors in balances, misapplied payments, and wrongful denials of payment plans.\\n- Issues with loan balances increasing unexpectedly due to mismanagement or improper handling of interest (e.g., forbearance steering leading to interest capitalization).\\n- Inaccurate or misleading credit reporting related to loan status.\\n- Lack of proper communication from servicers about loan status, default notices, or changes in repayment terms.\\n- Problems with accessing and verifying loan information, including identity or fraud concerns.\\n- Servicer misconduct such as neglecting borrower rights, mishandling documents, or failing to process requests for deferment, consolidation, or forgiveness.\\n\\nOverall, the most recurring theme appears to be **servicing-related issues**, such as mismanagement of payments, errors in balances, and failure to properly communicate or process borrower requests. \\n\\nIf I had to summarize the most common issue based on this data, it would be:  \\n**Problems with loan servicing, including mishandling of payments, balances, and communication.**  \\n\\nIf you need a specific aspect or more detailed explanation, please let me know!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "aAlSthxrRDBC",
    "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, it appears that many complaints were handled in a timely manner, with responses marked as \"Yes\" for timely response, and some complaints explicitly stating responses occurred within required timeframes. \\n\\nHowever, there are also several complaints where responses were delayed or not received within the expected time, including cases marked as \"No\" for timely response, where complainants experienced wait times exceeding 15 days or where no response was received for over a year.\\n\\nTherefore, the answer is: **Yes, some complaints did not get handled in a timely manner.**'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Uv1mpCK8REs4",
    "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"People failed to pay back their loans for various reasons, including:\\n\\n1. Lack of clear information about the true cost of loans and interest accrual, leading borrowers to underestimate how much they owed.\\n2. Difficulty managing interest that continued to accumulate during forbearance or deferment, making loans harder to pay off over time.\\n3. Limited or confusing repayment options, with some borrowers not qualifying for loan forgiveness programs and feeling misled about available assistance.\\n4. Financial hardships such as unemployment, medical issues, or homelessness, which made payments impossible.\\n5. Challenges with loan servicers providing inaccurate or misleading information, poor communication, or mishandling accounts, which hindered borrowers' ability to manage or understand their loans.\\n6. Errors or discrepancies in loan reporting and account status that negatively impacted credit scores and created additional obstacles to repayment.\\n7. Experiences of being pushed into long-term forbearance or forced into consolidations without being informed of better options like income-driven repayment or loan rehabilitation.\\n8. Systemic issues within loan servicing and transfer processes, including unauthorized account transfers, lack of documentation, or improper handling, which contributed to confusion and default.\\n9. Specific situations such as inability to return to work after injury or illness, homelessness, or initial unawareness of student loan obligations.\\n\\nOverall, the failure to pay back loans was often related to a combination of financial hardship, lack of transparent information, systemic servicing issues, and the complex or confusing nature of student loan management.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### ‚ùì Question #2:\n",
    "\n",
    "Explain how generating multiple reformulations of a user query can improve recall.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### Answer:\n",
    "\n",
    "- A single user query might miss relevant documents due to vocabulary mismatch. For example, if a user asks \"payment problems\", the LLM might generate variations like billing issues, transaction errors, payment processing difficulties, or account charge problems. By combining results from all query variations, you get the union of relevant documents rather than being limited to what a single query formulation can retrieve. This significantly improves recall at the potential cost of some precision.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDEawBf_d_3G"
   },
   "source": [
    "## Task 8: Parent Document Retriever\n",
    "\n",
    "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
    "\n",
    "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
    "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
    "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
    "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
    "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
    "\n",
    "Okay, maybe that was a few steps - but the basic idea is this:\n",
    "\n",
    "- Search for small documents\n",
    "- Return big documents\n",
    "\n",
    "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
    "\n",
    "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qJ53JJuMd_ZH"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "parent_docs = loan_complaint_data\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOpXfVUH3gL3"
   },
   "source": [
    "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
    "\n",
    "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzFc-_9HlGQ-",
    "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
   },
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_g95FA3s6w"
   },
   "source": [
    "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "BpWVjPf4fLUp"
   },
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoYmSWfE32Zo"
   },
   "source": [
    "By default, this is empty as we haven't added any documents - let's add some now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "iQ2ZzfKigMZc"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever.add_documents(parent_docs, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI7Tip1335rE"
   },
   "source": [
    "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Qq_adt2KlSqp"
   },
   "outputs": [],
   "source": [
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNolUVQb4Apt"
   },
   "source": [
    "Let's give it a whirl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "TXB5i89Zly5W",
    "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common issue with loans, based on the provided complaints, appears to be problems related to mismanagement and lack of transparency. Specific issues include:\\n\\n- Struggling to repay loans due to financial hardship and inadequate information about loan terms or consequences.\\n- Receiving bad or incomplete information about loans from lenders or servicers.\\n- Unauthorized or unexpected changes to loan terms, such as interest rate increases, incorrect balances, or lack of disclosure during processes like loan consolidation.\\n- Systemic breakdowns impacting credit reporting and loan servicing, leading to errors and negative credit score impacts.\\n\\nIn summary, the most common issue involves borrowers facing difficulties because of insufficient or misleading information, mismanagement, or systemic errors in loan handling and reporting.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V5F1T-wNl3cg",
    "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, yes, several complaints indicate that issues were not handled in a timely manner. For example:\\n\\n- One complaint from 03/28/25 mentions that the consumer was told their application would be expedited and they would be contacted within 15 days, but as of the date of the complaint, no one had reached out.\\n- Multiple complaints from 04/11/25 describe long wait times (up to 7 hours or more) to speak with representatives and mention that issues remained unresolved or unaddressed for extended periods.\\n- Also, a complaint from 04/24/25 highlights repeated failed attempts to correct errors, with no resolution and ongoing difficulties, indicating delays and unresponsiveness.\\n\\nFurthermore, the metadata shows that for some complaints the response was marked \"No\" for timely response, which suggests that some issues did not get handled promptly.\\n\\nTherefore, yes, some complaints were not handled in a timely manner.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ZqARszGzvGcG",
    "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans primarily due to financial hardship caused by inadequate post-graduation employment prospects, misrepresentations about the value of their education, and lack of transparency from the educational institutions and loan servicers. For example, one complainant was unable to secure employment in their field after attending a college that closed and did not provide real career support, leading to severe financial difficulties and difficulty in making loan payments. Others experienced issues such as being required to start payments before the grace period ended, or having their payment obligations miscommunicated or improperly managed by loan servicers. Additionally, some faced complications like unverified or disputed debt reporting, which further hindered their ability to manage or repay their loans. Overall, these issues highlight a combination of systemic mismanagement, lack of clear information, and personal financial hardships as reasons why individuals failed to pay back their loans.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B41cj42s4DPM"
   },
   "source": [
    "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUrIBKl_TwS9"
   },
   "source": [
    "## Task 9: Ensemble Retriever\n",
    "\n",
    "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
    "\n",
    "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
    "\n",
    "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "8j7jpZsKTxic"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpo9Psl5hhJ-"
   },
   "source": [
    "We'll pack *all* of these retrievers together in an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "KZ__EZwpUKkd"
   },
   "outputs": [],
   "source": [
    "ensemble_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSsvHpRMj24L"
   },
   "source": [
    "Let's look at our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lMvqL88UQI-",
    "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common issues with loans, based on the complaints data, include:\\n\\n- Dealing with lender or servicer issues such as mismanagement, incorrect reporting, or poor communication.\\n- Problems with how payments are being handled, including inability to allocate payments properly or being steered into high-interest forbearances.\\n- Errors and inaccuracies in loan balances, interest charges, or credit reporting.\\n- Lack of transparency or proper documentation, including missing or improperly managed loan records.\\n- Problems related to loan forgiveness, discharge, or discharge eligibility.\\n- Unauthorized transfer of loans and improper handling of loan data.\\n- Poor customer service and failure to resolve disputes effectively.\\n- Issues with interest accrual and improper interest charges.\\n- Misleading or incomplete information about loan terms and repayment options.\\n\\nIn summary, the most common and recurring issue appears to be the mishandling of loan servicing, including mismanagement, misreporting, and inadequate communication, which causes financial harm and frustration among borrowers.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "MNFWLYECURI1",
    "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, yes, some complaints indicate that issues were not handled in a timely manner. For example:\\n\\n- Complaint ID 12973003 (from EdFinancial Services, NJ): The customer mentioned that it is over 2-3 weeks and they are still experiencing the same issue, which suggests a delay beyond the promised timeframe.\\n- Complaint ID 12935889 (from MOHELA, CO): The complainant was dissatisfied with the response time, as their complaint was not resolved promptly.\\n- Complaint ID 12410063 (from EdFinancial Services, CA): The customer reported ongoing struggles over nearly two and a half years, with repeated follow-ups and delays in resolution.\\n\\nAdditionally, some complaints explicitly mention delays or failure to resolve issues promptly, indicating that not all complaints were handled in a timely manner.\\n\\nIf you need a specific definitive answer: yes, some complaints did not get handled in a timely manner.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "A7qbHfWgUR4c",
    "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans for several reasons, including:\\n\\n- Lack of clear communication from loan servicers about when payments were expected to resume or if the loan had been transferred to a new servicer.\\n- Difficulty in understanding or managing complex repayment options, interest accumulation, and the impact of deferment or forbearance.\\n- Being misled by servicers into forbearance or consolidation without proper explanation of potential consequences like interest capitalization or loss of forgiveness eligibility.\\n- Changes in loan transfer status without notification, leading to unawareness of repayment obligations.\\n- Financial hardships that made monthly payments unaffordable, combined with insufficient support or guidance from lenders.\\n- Errors or inaccuracies in loan balance reporting, debt management, or credit reporting, causing confusion and financial hardship.\\n- Technical issues, such as trouble with online portals, misapplied payments, or inaccurate account status updates.\\n- Lack of access to or awareness of income-driven repayment or forgiveness programs.\\n\\nIn essence, many borrowers struggled to repay their loans due to systemic issues like poor communication, mismanagement, and complex loan servicing practices that did not adequately support or inform them about their repayment options or status.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MopbkNJAXVaN"
   },
   "source": [
    "## Task 10: Semantic Chunking\n",
    "\n",
    "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
    "\n",
    "Essentially, Semantic Chunking is implemented by:\n",
    "\n",
    "1. Embedding all sentences in the corpus.\n",
    "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
    "  - `percentile`\n",
    "  - `standard_deviation`\n",
    "  - `interquartile`\n",
    "  - `gradient`\n",
    "3. Each sequence of related sentences is kept as a document!\n",
    "\n",
    "Let's see how to implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9ciZbFEldv_"
   },
   "source": [
    "We'll use the `percentile` thresholding method for this example which will:\n",
    "\n",
    "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "66EIEWiEYl5y"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqoKmz12mhRW"
   },
   "source": [
    "Now we can split our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ROcV7o68ZIq7"
   },
   "outputs": [],
   "source": [
    "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8-LNC-Xmjex"
   },
   "source": [
    "Let's create a new vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "h3sl9QjyZhIe"
   },
   "outputs": [],
   "source": [
    "semantic_vectorstore = Qdrant.from_documents(\n",
    "    semantic_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh_r_-LHmmKn"
   },
   "source": [
    "We'll use naive retrieval for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "odVyDUHwZftc"
   },
   "outputs": [],
   "source": [
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkeiv_ojmp6G"
   },
   "source": [
    "Finally we can create our classic chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "xWE_0J0mZveG"
   },
   "outputs": [],
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5pfjLQ3ms9_"
   },
   "source": [
    "And view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lN2j-e4Z0SD",
    "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints data, the most common issue with loans appears to be problems related to loan servicing and communication issues. Specific recurring issues include:\\n\\n- Trouble with how payments are being handled, such as incorrect payment amounts or re-amortization delays.\\n- Difficulties in obtaining clear or accurate information about loan status, balances, or payment plans.\\n- Problems with reporting and account status errors, such as loans being reported in default or delinquency falsely.\\n- Issues with auto-debit setup and verification.\\n- Concerns over improper reporting, breach of privacy, or illegal collection practices.\\n\\nOverall, challenges related to loan management, including lack of transparency, miscommunication, and administrative errors, seem to be the most prevalent issues with loans in this dataset.\\n\\nIf you need a specific summary or further details, please let me know!'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "xdqfBH1SZ3f9",
    "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, it appears that some complaints did not get handled in a timely manner. Specifically, multiple complaints state that the companies responded with \"Closed with explanation\" and there is no indication that they addressed the issues within the expected timeframe. \\n\\nFor example:\\n- The complaint about Nelnet regarding unresponded letters and alleged misconduct indicates no subsequent response to the consumer\\'s inquiries.\\n- Several complaints mention that, despite the companies\\' responses, the issues remain unresolved, and the consumers continue to face problems such as incorrect billing, unauthorized reporting, or lack of communication.\\n\\nWhile some responses are marked \"Yes\" for timely response, the lack of resolution or ongoing disputes suggests that not all complaints were effectively handled in a timely manner. \\n\\nTherefore, yes, some complaints did not get handled in a timely manner.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "rAcAObZnZ4o6",
    "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans for various reasons, including issues with loan servicing, miscommunication, or administrative problems. Some specific reasons highlighted in the complaints include:\\n\\n- Receiving bad or incomplete information about their loan status or repayment terms.\\n- Administrative delays or errors, such as failure to re-amortize payments after forbearance or missing payment records.\\n- Disputes over the legitimacy of the debt or treatment of their loan accounts.\\n- Problems with loan documentation or certification, leading to delays or complications in forgiveness or discharge.\\n- Allegations of illegal reporting or collection practices, which may contribute to confusion or default status errors.\\n- Personal financial difficulties, while not explicitly detailed, can also be implied as a general reason for default if borrowers are unable to meet payments.\\n\\nIn summary, failure to pay back loans was often related to administrative errors, communication issues, or legal disputes over the legitimacy and reporting of the loans.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### ‚ùì Question #3:\n",
    "\n",
    "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### Answer:\n",
    "Short and repetitive sentences like FAQ entries will have very similar embeddings, resulting in minimal semantic distances between consecutive sentences. FAQ structures depend on maintaining clear Q&A pairs, but semantic chunking might merge multiple Q&A pairs if they're topically similar.\n",
    "\n",
    "The adjustments to make for the algorithm is to lower the threshold by using a more sensitive breakpoint to detect subtle semantic shifts between different FAQ topics. Additionally, implement custom logic to respect FAQ formatting patterns before applying semantic chunking. Lastly, combine semantic chunking with rule-based splitting that preserves Q&A pair integrity.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk2n3-pnVWDJ"
   },
   "source": [
    "# ü§ù Breakout Room Part #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SkJLYwMVZkj"
   },
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### üèóÔ∏è Activity #1\n",
    "\n",
    "Your task is to evaluate the various Retriever methods against each other. \n",
    "You can use the loans or bills dataset.\n",
    "\n",
    "You are expected to:\n",
    "\n",
    "1. Create a \"golden dataset\"\n",
    " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
    "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
    " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
    "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
    "\n",
    "Your analysis should factor in:\n",
    "  - Cost\n",
    "  - Latency\n",
    "  - Performance\n",
    "\n",
    "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWAr16a5XMub"
   },
   "source": [
    "##### HINTS:\n",
    "\n",
    "- LangSmith provides detailed information about latency and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NLTK Import\n",
    "\n",
    "To prevent errors that may occur based on OS - we'll import NLTK and download the needed packages to ensure correct handling of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andrei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Andrei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dependencies and API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import getpass\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key: \")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key: \")\n",
    "\n",
    "# Enable LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"AI/LLM Engineering\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "\n",
    "path = \"bills/\"\n",
    "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "rag_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333ff487ed504066b199a63d416097ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02f1c075b004ac6afc15c8fcb2096ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8361f49a3366470d8a5dd9ec025edfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2371c2e04e9410d960cfaa7c5b36b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65aa046010b44067b9905bfb070c1d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b086c706874133a80f27eaf6c1e83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8a2c5f3fc944038ded648096b78cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.testset.graph import KnowledgeGraph, Node, NodeType\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer, MultiHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "for doc in docs[:20]:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "        )\n",
    "    )\n",
    "\n",
    "transformer_llm = generator_llm\n",
    "embedding_model = generator_embeddings\n",
    "default_transforms_list = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
    "apply_transforms(kg, default_transforms_list)\n",
    "\n",
    "kg.save(\"bills/ai_law.json\")\n",
    "bills_data_kg = KnowledgeGraph.load(\"bills/ai_law.json\")\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=bills_data_kg)\n",
    "\n",
    "query_distribution = [\n",
    "        (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
    "        (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25),\n",
    "        (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25),\n",
    "]\n",
    "\n",
    "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
    "testset_df = testset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Vectorstore Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    rag_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"MainVectorStore\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Retrievers Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BM25 Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(rag_documents, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parent Document Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"parent_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"parent_documents\", \n",
    "    embedding=embeddings, \n",
    "    client=client\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "\n",
    "parent_document_retriever.add_documents(parent_docs, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. RAG Chain Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "def create_rag_chain(retriever):\n",
    "    return (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "\n",
    "rag_chains = {\n",
    "    \"naive\": create_rag_chain(naive_retriever),\n",
    "    \"bm25\": create_rag_chain(bm25_retriever),\n",
    "    \"reranking\": create_rag_chain(compression_retriever),\n",
    "    \"multi_query\": create_rag_chain(multi_query_retriever),\n",
    "    \"parent_document\": create_rag_chain(parent_document_retriever),\n",
    "    \"ensemble\": create_rag_chain(ensemble_retriever)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Ragas Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.evaluation import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    "    answer_relevancy\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_chains(rag_chains, testset_df):\n",
    "    \"\"\"\n",
    "    Evaluate each RAG chain using RAGAS metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for chain_name, rag_chain in rag_chains.items():\n",
    "        print(f\"\\nüîÑ Evaluating {chain_name}...\")\n",
    "        \n",
    "        # Create dataset for this specific chain\n",
    "        chain_data = []\n",
    "        \n",
    "        for _, row in testset_df.iterrows():\n",
    "            question = row['user_input']\n",
    "            ground_truth = row['reference']\n",
    "            \n",
    "            try:\n",
    "                # Get response from RAG chain\n",
    "                response = rag_chain.invoke({\"question\": question})\n",
    "                \n",
    "                # Extract answer and contexts\n",
    "                answer = response[\"response\"].content\n",
    "                contexts = [doc.page_content for doc in response[\"context\"]]\n",
    "                \n",
    "                chain_data.append({\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                    'contexts': contexts,\n",
    "                    'ground_truth': ground_truth\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing question with {chain_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not chain_data:\n",
    "            print(f\"No valid data for {chain_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Convert to Dataset format for RAGAS\n",
    "        dataset = Dataset.from_list(chain_data)\n",
    "        \n",
    "        # Evaluate using RAGAS\n",
    "        try:\n",
    "            evaluation_result = evaluate(\n",
    "                dataset=dataset,\n",
    "                metrics=[context_precision, context_recall, faithfulness, answer_relevancy],\n",
    "                llm=generator_llm,\n",
    "                embeddings=generator_embeddings\n",
    "            )\n",
    "\n",
    "            # Helper function to safely extract and round metrics\n",
    "            def safe_round(value, decimals=4):\n",
    "                if isinstance(value, (list, tuple)):\n",
    "                    # If it's a list, take the mean\n",
    "                    return round(sum(value) / len(value), decimals) if value else 0.0\n",
    "                elif isinstance(value, (int, float)):\n",
    "                    return round(value, decimals)\n",
    "                else:\n",
    "                    return 0.0\n",
    "            \n",
    "            results.append({\n",
    "                \"Retriever\": chain_name,\n",
    "                \"Context Precision\": safe_round(evaluation_result[\"context_precision\"]),\n",
    "                \"Context Recall\": safe_round(evaluation_result[\"context_recall\"]),\n",
    "                \"Faithfulness\": safe_round(evaluation_result[\"faithfulness\"]),\n",
    "                \"Answer Relevancy\": safe_round(evaluation_result[\"answer_relevancy\"])\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úÖ {chain_name} evaluation complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed for {chain_name}: {e}\")\n",
    "            results.append({\n",
    "                \"Retriever\": chain_name,\n",
    "                \"Context Precision\": \"Error\",\n",
    "                \"Context Recall\": \"Error\", \n",
    "                \"Faithfulness\": \"Error\",\n",
    "                \"Answer Relevancy\": \"Error\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_latency_and_cost(rag_chains, testset_df, num_samples=5):\n",
    "    \"\"\"\n",
    "    Analyze latency and cost for each RAG chain\n",
    "    \"\"\"\n",
    "    performance_results = []\n",
    "    \n",
    "    # Sample questions for latency testing\n",
    "    sample_questions = testset_df['user_input'].head(num_samples).tolist()\n",
    "    \n",
    "    for chain_name, rag_chain in rag_chains.items():\n",
    "        print(f\"Testing latency for {chain_name}...\")\n",
    "        \n",
    "        total_time = 0\n",
    "        successful_runs = 0\n",
    "        \n",
    "        for question in sample_questions:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                response = rag_chain.invoke({\"question\": question})\n",
    "                end_time = time.time()\n",
    "                \n",
    "                total_time += (end_time - start_time)\n",
    "                successful_runs += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in {chain_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        avg_latency = total_time / successful_runs if successful_runs > 0 else float('inf')\n",
    "        \n",
    "        # Cost estimation based on retriever complexity\n",
    "        cost_estimates = {\n",
    "            \"naive\": \"Low\",\n",
    "            \"bm25\": \"Very Low\", \n",
    "            \"reranking\": \"High\",  # Uses Cohere reranking API\n",
    "            \"multi_query\": \"Medium-High\",  # Multiple LLM calls for query generation\n",
    "            \"parent_document\": \"Low-Medium\",\n",
    "            \"ensemble\": \"Very High\",  # Combines all retrievers\n",
    "            \"semantic\": \"Low-Medium\"\n",
    "        }\n",
    "        \n",
    "        performance_results.append({\n",
    "            \"Retriever\": chain_name,\n",
    "            \"Average Latency (s)\": round(avg_latency, 3),\n",
    "            \"Success Rate\": f\"{successful_runs}/{num_samples}\",\n",
    "            \"Estimated Cost\": cost_estimates.get(chain_name, \"Unknown\")\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(performance_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluating naive...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071e9c77f3384ba292c890a188c8734b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ naive evaluation complete!\n",
      "\n",
      "üîÑ Evaluating bm25...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4f1f97cd144f4580acecc4aba38571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ bm25 evaluation complete!\n",
      "\n",
      "üîÑ Evaluating reranking...\n",
      "Error processing question with reranking: status_code: 429, body: data=None id='8e72c1a4-0909-4193-8c1f-587c81d4e273' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0615ff6227e1479c88e476112522bbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ reranking evaluation complete!\n",
      "\n",
      "üîÑ Evaluating multi_query...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab74cb4da774de5ba8fb1a53106807b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ multi_query evaluation complete!\n",
      "\n",
      "üîÑ Evaluating parent_document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb98359d3174ab190b7bea746b97598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ parent_document evaluation complete!\n",
      "\n",
      "üîÑ Evaluating ensemble...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4151c0d1456f467f921907a211f25a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ensemble evaluation complete!\n",
      "Testing latency for naive...\n",
      "Testing latency for bm25...\n",
      "Testing latency for reranking...\n",
      "Testing latency for multi_query...\n",
      "Testing latency for parent_document...\n",
      "Testing latency for ensemble...\n"
     ]
    }
   ],
   "source": [
    "ragas_results = evaluate_rag_chains(rag_chains, testset_df)\n",
    "performance_results = analyze_latency_and_cost(rag_chains, testset_df)\n",
    "\n",
    "final_analysis = ragas_results.merge(\n",
    "    performance_results[[\"Retriever\", \"Average Latency (s)\", \"Estimated Cost\"]], \n",
    "    on=\"Retriever\", \n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Display and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üéØ COMPREHENSIVE RETRIEVER EVALUATION RESULTS\n",
      "====================================================================================================\n",
      "\n",
      "üìä RAGAS Metrics Results:\n",
      "      Retriever  Context Precision  Context Recall  Faithfulness  Answer Relevancy\n",
      "          naive             0.3964          0.8182        0.8840            0.8521\n",
      "           bm25             0.3182          0.6667        0.7740            0.6970\n",
      "      reranking             0.2333          0.4833        0.8981            0.7667\n",
      "    multi_query             0.3449          0.8333        0.8452            0.8617\n",
      "parent_document             0.3763          0.4848        0.7138            0.6105\n",
      "       ensemble             0.4721          0.9091        0.9318            0.9499\n",
      "\n",
      "‚ö° Performance Analysis:\n",
      "      Retriever  Average Latency (s) Success Rate Estimated Cost\n",
      "          naive                2.834          5/5            Low\n",
      "           bm25                1.664          5/5       Very Low\n",
      "      reranking                3.154          5/5           High\n",
      "    multi_query                5.408          5/5    Medium-High\n",
      "parent_document                1.828          5/5     Low-Medium\n",
      "       ensemble                7.538          5/5      Very High\n",
      "\n",
      "üîç Combined Analysis:\n",
      "      Retriever  Context Precision  Context Recall  Faithfulness  Answer Relevancy  Average Latency (s) Estimated Cost\n",
      "          naive             0.3964          0.8182        0.8840            0.8521                2.834            Low\n",
      "           bm25             0.3182          0.6667        0.7740            0.6970                1.664       Very Low\n",
      "      reranking             0.2333          0.4833        0.8981            0.7667                3.154           High\n",
      "    multi_query             0.3449          0.8333        0.8452            0.8617                5.408    Medium-High\n",
      "parent_document             0.3763          0.4848        0.7138            0.6105                1.828     Low-Medium\n",
      "       ensemble             0.4721          0.9091        0.9318            0.9499                7.538      Very High\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üéØ COMPREHENSIVE RETRIEVER EVALUATION RESULTS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nüìä RAGAS Metrics Results:\")\n",
    "print(ragas_results.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚ö° Performance Analysis:\")\n",
    "print(performance_results.to_string(index=False))\n",
    "\n",
    "print(\"\\nüîç Combined Analysis:\")\n",
    "print(final_analysis.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### Analysis & Observations:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results_df):\n",
    "    \"\"\"\n",
    "    Provide detailed analysis and recommendations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DETAILED ANALYSIS & RECOMMENDATIONS\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Find best performers in each category\n",
    "    numeric_cols = [\"Context Precision\", \"Context Recall\", \"Faithfulness\", \"Answer Relevancy\"]\n",
    "    \n",
    "    print(\"\\nBEST PERFORMERS BY METRIC:\")\n",
    "    for col in numeric_cols:\n",
    "        if col in results_df.columns:\n",
    "            numeric_values = pd.to_numeric(results_df[col], errors='coerce')\n",
    "            if not numeric_values.isna().all():\n",
    "                best_idx = numeric_values.idxmax()\n",
    "                best_retriever = results_df.loc[best_idx, \"Retriever\"]\n",
    "                best_score = numeric_values.max()\n",
    "                print(f\"  {col}: {best_retriever} ({best_score:.4f})\")\n",
    "    \n",
    "    # Calculate overall performance scores\n",
    "    print(f\"\\nOVERALL PERFORMANCE RANKING:\")\n",
    "    print(f\"{'Rank':<5} {'Retriever':<15} {'Avg Score':<12} {'Cost':<15} {'Latency':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    performance_scores = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        scores = []\n",
    "        for col in numeric_cols:\n",
    "            if col in row:\n",
    "                score = pd.to_numeric(row[col], errors='coerce')\n",
    "                if not pd.isna(score):\n",
    "                    scores.append(score)\n",
    "        \n",
    "        avg_score = sum(scores) / len(scores) if scores else 0\n",
    "        performance_scores.append((row['Retriever'], avg_score, row.get('Estimated Cost', 'N/A'), row.get('Average Latency (s)', 'N/A')))\n",
    "    \n",
    "    # Sort by average score\n",
    "    performance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (retriever, avg_score, cost, latency) in enumerate(performance_scores, 1):\n",
    "        print(f\"{i:<5} {retriever:<15} {avg_score:<12.4f} {cost:<15} {latency:<12}\")\n",
    "    \n",
    "    # Recommendations based on use case\n",
    "    print(f\"\\nRECOMMENDATIONS BY USE CASE:\")\n",
    "    \n",
    "    print(f\"\\n1. HIGHEST ACCURACY (regardless of cost):\")\n",
    "    best_accuracy = performance_scores[0][0]\n",
    "    print(f\"   ‚Üí Use '{best_accuracy}' for maximum performance\")\n",
    "    \n",
    "    print(f\"\\n2. COST-CONSCIOUS (good performance, low cost):\")\n",
    "    cost_efficient = [x for x in performance_scores if x[2] in ['Low', 'Very Low']]\n",
    "    if cost_efficient:\n",
    "        print(f\"   ‚Üí Use '{cost_efficient[0][0]}' for balanced cost/performance\")\n",
    "    \n",
    "    print(f\"\\n3. SPEED-OPTIMIZED (fastest response time):\")\n",
    "    latency_data = [(x[0], x[3]) for x in performance_scores if isinstance(x[3], (int, float))]\n",
    "    if latency_data:\n",
    "        fastest = min(latency_data, key=lambda x: x[1])\n",
    "        print(f\"   ‚Üí Use '{fastest[0]}' for fastest responses ({fastest[1]}s)\")\n",
    "    \n",
    "    print(f\"\\n4. PRODUCTION (balanced for enterprise use):\")\n",
    "    production_candidates = [x for x in performance_scores if x[2] in ['Low', 'Medium', 'Low-Medium'] and x[1] > 0.6]\n",
    "    if production_candidates:\n",
    "        print(f\"   ‚Üí Use '{production_candidates[0][0]}' for production deployment\")\n",
    "    \n",
    "    print(f\"\\nKEY INSIGHTS:\")\n",
    "    print(f\"‚Ä¢ Context Precision measures how relevant retrieved chunks are\")\n",
    "    print(f\"‚Ä¢ Context Recall measures completeness of retrieved information\")  \n",
    "    print(f\"‚Ä¢ Faithfulness measures if answers are grounded in context\")\n",
    "    print(f\"‚Ä¢ Answer Relevancy measures if answers address the question\")\n",
    "    print(f\"‚Ä¢ Ensemble methods typically have highest cost but may provide best accuracy\")\n",
    "    print(f\"‚Ä¢ BM25 is often most cost-effective for keyword-heavy queries\")\n",
    "    print(f\"‚Ä¢ Reranking adds cost but can significantly improve precision\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "DETAILED ANALYSIS & RECOMMENDATIONS\n",
      "====================================================================================================\n",
      "\n",
      "üèÜ BEST PERFORMERS BY METRIC:\n",
      "  Context Precision: ensemble (0.4721)\n",
      "  Context Recall: ensemble (0.9091)\n",
      "  Faithfulness: ensemble (0.9318)\n",
      "  Answer Relevancy: ensemble (0.9499)\n",
      "\n",
      "OVERALL PERFORMANCE RANKING:\n",
      "Rank  Retriever       Avg Score    Cost            Latency     \n",
      "----------------------------------------------------------------------\n",
      "1     ensemble        0.8157       Very High       7.538       \n",
      "2     naive           0.7377       Low             2.834       \n",
      "3     multi_query     0.7213       Medium-High     5.408       \n",
      "4     bm25            0.6140       Very Low        1.664       \n",
      "5     reranking       0.5954       High            3.154       \n",
      "6     parent_document 0.5464       Low-Medium      1.828       \n",
      "\n",
      "RECOMMENDATIONS BY USE CASE:\n",
      "\n",
      "1. HIGHEST ACCURACY (regardless of cost):\n",
      "   ‚Üí Use 'ensemble' for maximum performance\n",
      "\n",
      "2. COST-CONSCIOUS (good performance, low cost):\n",
      "   ‚Üí Use 'naive' for balanced cost/performance\n",
      "\n",
      "3. SPEED-OPTIMIZED (fastest response time):\n",
      "   ‚Üí Use 'bm25' for fastest responses (1.664s)\n",
      "\n",
      "4. PRODUCTION (balanced for enterprise use):\n",
      "   ‚Üí Use 'naive' for production deployment\n",
      "\n",
      "KEY INSIGHTS:\n",
      "‚Ä¢ Context Precision measures how relevant retrieved chunks are\n",
      "‚Ä¢ Context Recall measures completeness of retrieved information\n",
      "‚Ä¢ Faithfulness measures if answers are grounded in context\n",
      "‚Ä¢ Answer Relevancy measures if answers address the question\n",
      "‚Ä¢ Ensemble methods typically have highest cost but may provide best accuracy\n",
      "‚Ä¢ BM25 is often most cost-effective for keyword-heavy queries\n",
      "‚Ä¢ Reranking adds cost but can significantly improve precision\n"
     ]
    }
   ],
   "source": [
    "analyze_results(final_analysis)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "13-advanced-retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
